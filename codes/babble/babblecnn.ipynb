{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the input data\n",
      "(256788, 129, 16, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 123, 10, 64)       3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 123, 10, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 123, 10, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 119, 6, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 119, 6, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 119, 6, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 117, 4, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 117, 4, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 117, 4, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 117, 4, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 117, 4, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 117, 4, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 119, 6, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 119, 6, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 119, 6, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 123, 10, 64)       204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 123, 10, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 123, 10, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 129, 16, 1)        3137      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 129, 16, 1)        4         \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 129, 16, 1)        0         \n",
      "=================================================================\n",
      "Total params: 1,075,717\n",
      "Trainable params: 1,073,923\n",
      "Non-trainable params: 1,794\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 254220 samples, validate on 2568 samples\n",
      "Epoch 1/1000\n",
      "254220/254220 [==============================] - 138s 542us/step - loss: 5.6666 - acc: 1.7152e-08 - val_loss: 5.4077 - val_acc: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "254220/254220 [==============================] - 138s 541us/step - loss: 5.6532 - acc: 1.9058e-08 - val_loss: 5.3199 - val_acc: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.6478 - acc: 2.0964e-08 - val_loss: 5.4294 - val_acc: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "254220/254220 [==============================] - 138s 543us/step - loss: 5.6380 - acc: 2.6681e-08 - val_loss: 5.4946 - val_acc: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "254220/254220 [==============================] - 138s 544us/step - loss: 5.6320 - acc: 2.6681e-08 - val_loss: 6.1604 - val_acc: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "254220/254220 [==============================] - 137s 540us/step - loss: 5.6289 - acc: 2.2870e-08 - val_loss: 5.3193 - val_acc: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "254220/254220 [==============================] - 138s 544us/step - loss: 5.6244 - acc: 2.6681e-08 - val_loss: 5.3728 - val_acc: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "254220/254220 [==============================] - 138s 542us/step - loss: 5.6206 - acc: 2.8587e-08 - val_loss: 5.3619 - val_acc: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "254220/254220 [==============================] - 138s 541us/step - loss: 5.6177 - acc: 2.8587e-08 - val_loss: 5.2784 - val_acc: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "254220/254220 [==============================] - 138s 542us/step - loss: 5.6124 - acc: 2.8587e-08 - val_loss: 5.4161 - val_acc: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "254220/254220 [==============================] - 138s 544us/step - loss: 5.6111 - acc: 2.6681e-08 - val_loss: 5.3688 - val_acc: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "254220/254220 [==============================] - 138s 541us/step - loss: 5.6075 - acc: 3.2399e-08 - val_loss: 5.2649 - val_acc: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "254220/254220 [==============================] - 138s 542us/step - loss: 5.6047 - acc: 3.4305e-08 - val_loss: 5.4836 - val_acc: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "246784/254220 [============================>.] - ETA: 4s - loss: 5.5967 - acc: 3.1412e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5659 - acc: 3.6210e-08 - val_loss: 5.2195 - val_acc: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5634 - acc: 3.6210e-08 - val_loss: 5.2301 - val_acc: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "254220/254220 [==============================] - 139s 545us/step - loss: 5.5637 - acc: 3.6210e-08 - val_loss: 5.2704 - val_acc: 0.0000e+00\n",
      "Epoch 36/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5606 - acc: 3.4305e-08 - val_loss: 5.2336 - val_acc: 0.0000e+00\n",
      "Epoch 37/1000\n",
      "254220/254220 [==============================] - 138s 545us/step - loss: 5.5603 - acc: 3.2399e-08 - val_loss: 5.2211 - val_acc: 0.0000e+00\n",
      "Epoch 38/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5596 - acc: 3.4305e-08 - val_loss: 5.2248 - val_acc: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5574 - acc: 3.6210e-08 - val_loss: 5.2198 - val_acc: 0.0000e+00\n",
      "Epoch 40/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5559 - acc: 3.4305e-08 - val_loss: 5.2345 - val_acc: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5545 - acc: 3.8116e-08 - val_loss: 5.2131 - val_acc: 0.0000e+00\n",
      "Epoch 42/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5547 - acc: 3.4305e-08 - val_loss: 5.2241 - val_acc: 0.0000e+00\n",
      "Epoch 43/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5536 - acc: 3.6210e-08 - val_loss: 5.2220 - val_acc: 0.0000e+00\n",
      "Epoch 44/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5518 - acc: 3.6210e-08 - val_loss: 5.2219 - val_acc: 0.0000e+00\n",
      "Epoch 45/1000\n",
      "245760/254220 [============================>.] - ETA: 4s - loss: 5.5428 - acc: 3.9428e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5393 - acc: 3.6210e-08 - val_loss: 5.2324 - val_acc: 0.0000e+00\n",
      "Epoch 63/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5402 - acc: 3.6210e-08 - val_loss: 5.2203 - val_acc: 0.0000e+00\n",
      "Epoch 64/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5375 - acc: 3.8116e-08 - val_loss: 5.2326 - val_acc: 0.0000e+00\n",
      "Epoch 65/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5373 - acc: 3.6210e-08 - val_loss: 5.2388 - val_acc: 0.0000e+00\n",
      "Epoch 66/1000\n",
      "254220/254220 [==============================] - 138s 544us/step - loss: 5.5373 - acc: 3.6210e-08 - val_loss: 5.2283 - val_acc: 0.0000e+00\n",
      "Epoch 67/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5362 - acc: 4.1928e-08 - val_loss: 5.2474 - val_acc: 0.0000e+00\n",
      "Epoch 68/1000\n",
      "254220/254220 [==============================] - 139s 545us/step - loss: 5.5364 - acc: 3.8116e-08 - val_loss: 5.2192 - val_acc: 0.0000e+00\n",
      "Epoch 69/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5354 - acc: 3.8116e-08 - val_loss: 5.2239 - val_acc: 0.0000e+00\n",
      "Epoch 70/1000\n",
      "254220/254220 [==============================] - 139s 545us/step - loss: 5.5356 - acc: 3.4305e-08 - val_loss: 5.2188 - val_acc: 0.0000e+00\n",
      "Epoch 71/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5212 - acc: 3.6210e-08 - val_loss: 5.2140 - val_acc: 0.0000e+00\n",
      "Epoch 118/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5216 - acc: 3.8116e-08 - val_loss: 5.2147 - val_acc: 0.0000e+00\n",
      "Epoch 119/1000\n",
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5213 - acc: 3.6210e-08 - val_loss: 5.2225 - val_acc: 0.0000e+00\n",
      "Epoch 120/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5214 - acc: 3.8116e-08 - val_loss: 5.2176 - val_acc: 0.0000e+00\n",
      "Epoch 121/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5207 - acc: 3.8116e-08 - val_loss: 5.2221 - val_acc: 0.0000e+00\n",
      "Epoch 122/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5209 - acc: 3.6210e-08 - val_loss: 5.2180 - val_acc: 0.0000e+00\n",
      "Epoch 123/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5199 - acc: 3.8116e-08 - val_loss: 5.2213 - val_acc: 0.0000e+00\n",
      "Epoch 124/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5206 - acc: 3.4305e-08 - val_loss: 5.2376 - val_acc: 0.0000e+00\n",
      "Epoch 125/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5206 - acc: 3.8116e-08 - val_loss: 5.2319 - val_acc: 0.0000e+00\n",
      "Epoch 126/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5199 - acc: 3.8116e-08 - val_loss: 5.2214 - val_acc: 0.0000e+00\n",
      "Epoch 127/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5198 - acc: 3.8116e-08 - val_loss: 5.2173 - val_acc: 0.0000e+00\n",
      "Epoch 128/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5193 - acc: 3.6210e-08 - val_loss: 5.2153 - val_acc: 0.0000e+00\n",
      "Epoch 129/1000\n",
      "254220/254220 [==============================] - 139s 549us/step - loss: 5.5189 - acc: 3.4305e-08 - val_loss: 5.2158 - val_acc: 0.0000e+00\n",
      "Epoch 130/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5196 - acc: 3.8116e-08 - val_loss: 5.2217 - val_acc: 0.0000e+00\n",
      "Epoch 131/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5191 - acc: 3.6210e-08 - val_loss: 5.2226 - val_acc: 0.0000e+00\n",
      "Epoch 132/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5192 - acc: 3.6210e-08 - val_loss: 5.2362 - val_acc: 0.0000e+00\n",
      "Epoch 133/1000\n",
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5198 - acc: 3.6210e-08 - val_loss: 5.2403 - val_acc: 0.0000e+00\n",
      "Epoch 134/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5184 - acc: 3.8116e-08 - val_loss: 5.2148 - val_acc: 0.0000e+00\n",
      "Epoch 135/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5183 - acc: 3.8116e-08 - val_loss: 5.2545 - val_acc: 0.0000e+00\n",
      "Epoch 136/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5194 - acc: 3.6210e-08 - val_loss: 5.2286 - val_acc: 0.0000e+00\n",
      "Epoch 137/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5182 - acc: 3.6210e-08 - val_loss: 5.2407 - val_acc: 0.0000e+00\n",
      "Epoch 138/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5192 - acc: 3.8116e-08 - val_loss: 5.2237 - val_acc: 0.0000e+00\n",
      "Epoch 140/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5181 - acc: 3.6210e-08 - val_loss: 5.2897 - val_acc: 0.0000e+00\n",
      "Epoch 141/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5173 - acc: 3.2399e-08 - val_loss: 5.2215 - val_acc: 0.0000e+00\n",
      "Epoch 142/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5171 - acc: 3.8116e-08 - val_loss: 5.2260 - val_acc: 0.0000e+00\n",
      "Epoch 143/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5171 - acc: 3.6210e-08 - val_loss: 5.2192 - val_acc: 0.0000e+00\n",
      "Epoch 144/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5172 - acc: 3.8116e-08 - val_loss: 5.2159 - val_acc: 0.0000e+00\n",
      "Epoch 145/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5169 - acc: 3.8116e-08 - val_loss: 5.2246 - val_acc: 0.0000e+00\n",
      "Epoch 146/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5170 - acc: 3.6210e-08 - val_loss: 5.2198 - val_acc: 0.0000e+00\n",
      "Epoch 147/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5169 - acc: 3.8116e-08 - val_loss: 5.2330 - val_acc: 0.0000e+00\n",
      "Epoch 148/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5173 - acc: 3.8116e-08 - val_loss: 5.2133 - val_acc: 0.0000e+00\n",
      "Epoch 149/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5167 - acc: 3.8116e-08 - val_loss: 5.2323 - val_acc: 0.0000e+00\n",
      "Epoch 150/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5171 - acc: 3.6210e-08 - val_loss: 5.2277 - val_acc: 0.0000e+00\n",
      "Epoch 151/1000\n",
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5169 - acc: 3.6210e-08 - val_loss: 5.2345 - val_acc: 0.0000e+00\n",
      "Epoch 152/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5164 - acc: 3.4305e-08 - val_loss: 5.2370 - val_acc: 0.0000e+00\n",
      "Epoch 153/1000\n",
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5166 - acc: 3.4305e-08 - val_loss: 5.2192 - val_acc: 0.0000e+00\n",
      "Epoch 154/1000\n",
      "254220/254220 [==============================] - 139s 549us/step - loss: 5.5155 - acc: 3.6210e-08 - val_loss: 5.2195 - val_acc: 0.0000e+00\n",
      "Epoch 155/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5163 - acc: 3.2399e-08 - val_loss: 5.2201 - val_acc: 0.0000e+00\n",
      "Epoch 156/1000\n",
      "254220/254220 [==============================] - 139s 549us/step - loss: 5.5160 - acc: 3.6210e-08 - val_loss: 5.2184 - val_acc: 0.0000e+00\n",
      "Epoch 157/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5152 - acc: 3.6210e-08 - val_loss: 5.2210 - val_acc: 0.0000e+00\n",
      "Epoch 158/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5152 - acc: 3.8116e-08 - val_loss: 5.2342 - val_acc: 0.0000e+00\n",
      "Epoch 159/1000\n",
      "254220/254220 [==============================] - 139s 545us/step - loss: 5.5154 - acc: 3.2399e-08 - val_loss: 5.2577 - val_acc: 0.0000e+00\n",
      "Epoch 160/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5167 - acc: 3.4305e-08 - val_loss: 5.2221 - val_acc: 0.0000e+00\n",
      "Epoch 161/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5151 - acc: 3.8116e-08 - val_loss: 5.2442 - val_acc: 0.0000e+00\n",
      "Epoch 162/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5155 - acc: 3.6210e-08 - val_loss: 5.2168 - val_acc: 0.0000e+00\n",
      "Epoch 163/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5154 - acc: 3.8116e-08 - val_loss: 5.2218 - val_acc: 0.0000e+00\n",
      "Epoch 164/1000\n",
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5153 - acc: 3.4305e-08 - val_loss: 5.2375 - val_acc: 0.0000e+00\n",
      "Epoch 165/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5152 - acc: 3.8116e-08 - val_loss: 5.2237 - val_acc: 0.0000e+00\n",
      "Epoch 166/1000\n",
      "254220/254220 [==============================] - 139s 545us/step - loss: 5.5159 - acc: 3.6210e-08 - val_loss: 5.2205 - val_acc: 0.0000e+00\n",
      "Epoch 167/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5150 - acc: 3.8116e-08 - val_loss: 5.2295 - val_acc: 0.0000e+00\n",
      "Epoch 168/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5138 - acc: 3.6210e-08 - val_loss: 5.2235 - val_acc: 0.0000e+00\n",
      "Epoch 170/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5141 - acc: 3.8116e-08 - val_loss: 5.2178 - val_acc: 0.0000e+00\n",
      "Epoch 171/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5137 - acc: 3.6210e-08 - val_loss: 5.2145 - val_acc: 0.0000e+00\n",
      "Epoch 172/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5146 - acc: 3.6210e-08 - val_loss: 5.2225 - val_acc: 0.0000e+00\n",
      "Epoch 173/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5144 - acc: 3.8116e-08 - val_loss: 5.2179 - val_acc: 0.0000e+00\n",
      "Epoch 174/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5148 - acc: 3.8116e-08 - val_loss: 5.2323 - val_acc: 0.0000e+00\n",
      "Epoch 175/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5139 - acc: 3.8116e-08 - val_loss: 5.2297 - val_acc: 0.0000e+00\n",
      "Epoch 176/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5133 - acc: 3.6210e-08 - val_loss: 5.2166 - val_acc: 0.0000e+00\n",
      "Epoch 177/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5135 - acc: 3.6210e-08 - val_loss: 5.2394 - val_acc: 0.0000e+00\n",
      "Epoch 178/1000\n",
      "254220/254220 [==============================] - 139s 549us/step - loss: 5.5141 - acc: 3.6210e-08 - val_loss: 5.2156 - val_acc: 0.0000e+00\n",
      "Epoch 179/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5135 - acc: 3.8116e-08 - val_loss: 5.2150 - val_acc: 0.0000e+00\n",
      "Epoch 180/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5134 - acc: 3.8116e-08 - val_loss: 5.2322 - val_acc: 0.0000e+00\n",
      "Epoch 181/1000\n",
      "164864/254220 [==================>...........] - ETA: 48s - loss: 5.4994 - acc: 3.5265e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5122 - acc: 3.6210e-08 - val_loss: 5.2532 - val_acc: 0.0000e+00\n",
      "Epoch 197/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5125 - acc: 3.6210e-08 - val_loss: 5.2279 - val_acc: 0.0000e+00\n",
      "Epoch 198/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5121 - acc: 3.6210e-08 - val_loss: 5.2248 - val_acc: 0.0000e+00\n",
      "Epoch 199/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5127 - acc: 3.4305e-08 - val_loss: 5.2328 - val_acc: 0.0000e+00\n",
      "Epoch 200/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5125 - acc: 3.4305e-08 - val_loss: 5.2266 - val_acc: 0.0000e+00\n",
      "Epoch 201/1000\n",
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5128 - acc: 3.6210e-08 - val_loss: 5.2304 - val_acc: 0.0000e+00\n",
      "Epoch 202/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5117 - acc: 3.6210e-08 - val_loss: 5.2234 - val_acc: 0.0000e+00\n",
      "Epoch 203/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5120 - acc: 3.8116e-08 - val_loss: 5.2362 - val_acc: 0.0000e+00\n",
      "Epoch 204/1000\n",
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5113 - acc: 3.8116e-08 - val_loss: 5.2170 - val_acc: 0.0000e+00\n",
      "Epoch 205/1000\n",
      "254220/254220 [==============================] - 139s 549us/step - loss: 5.5119 - acc: 3.4305e-08 - val_loss: 5.2243 - val_acc: 0.0000e+00\n",
      "Epoch 206/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5118 - acc: 3.6210e-08 - val_loss: 5.2219 - val_acc: 0.0000e+00\n",
      "Epoch 207/1000\n",
      "254220/254220 [==============================] - 139s 549us/step - loss: 5.5111 - acc: 3.8116e-08 - val_loss: 5.2241 - val_acc: 0.0000e+00\n",
      "Epoch 208/1000\n",
      "235520/254220 [==========================>...] - ETA: 10s - loss: 5.5045 - acc: 3.2914e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5105 - acc: 3.6210e-08 - val_loss: 5.2235 - val_acc: 0.0000e+00\n",
      "Epoch 228/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5095 - acc: 3.8116e-08 - val_loss: 5.2565 - val_acc: 0.0000e+00\n",
      "Epoch 229/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5101 - acc: 3.8116e-08 - val_loss: 5.2224 - val_acc: 0.0000e+00\n",
      "Epoch 230/1000\n",
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5102 - acc: 3.8116e-08 - val_loss: 5.2211 - val_acc: 0.0000e+00\n",
      "Epoch 231/1000\n",
      "254220/254220 [==============================] - 139s 549us/step - loss: 5.5096 - acc: 3.6210e-08 - val_loss: 5.2409 - val_acc: 0.0000e+00\n",
      "Epoch 232/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5102 - acc: 3.6210e-08 - val_loss: 5.2220 - val_acc: 0.0000e+00\n",
      "Epoch 233/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5103 - acc: 3.6210e-08 - val_loss: 5.2220 - val_acc: 0.0000e+00\n",
      "Epoch 234/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5096 - acc: 3.0493e-08 - val_loss: 5.2269 - val_acc: 0.0000e+00\n",
      "Epoch 235/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5093 - acc: 4.0022e-08 - val_loss: 5.2183 - val_acc: 0.0000e+00\n",
      "Epoch 236/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5093 - acc: 4.0022e-08 - val_loss: 5.2234 - val_acc: 0.0000e+00\n",
      "Epoch 237/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5102 - acc: 4.0022e-08 - val_loss: 5.2271 - val_acc: 0.0000e+00\n",
      "Epoch 238/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5092 - acc: 4.0022e-08 - val_loss: 5.2175 - val_acc: 0.0000e+00\n",
      "Epoch 239/1000\n",
      "118784/254220 [=============>................] - ETA: 1:13 - loss: 5.4989 - acc: 4.0788e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5087 - acc: 3.2399e-08 - val_loss: 5.2229 - val_acc: 0.0000e+00\n",
      "Epoch 258/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5082 - acc: 3.8116e-08 - val_loss: 5.2248 - val_acc: 0.0000e+00\n",
      "Epoch 259/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5085 - acc: 3.4305e-08 - val_loss: 5.2367 - val_acc: 0.0000e+00\n",
      "Epoch 260/1000\n",
      "254220/254220 [==============================] - 140s 551us/step - loss: 5.5089 - acc: 3.8116e-08 - val_loss: 5.2213 - val_acc: 0.0000e+00\n",
      "Epoch 261/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5086 - acc: 3.4305e-08 - val_loss: 5.2240 - val_acc: 0.0000e+00\n",
      "Epoch 262/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5079 - acc: 3.6210e-08 - val_loss: 5.2191 - val_acc: 0.0000e+00\n",
      "Epoch 263/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5078 - acc: 3.8116e-08 - val_loss: 5.2275 - val_acc: 0.0000e+00\n",
      "Epoch 264/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5083 - acc: 3.6210e-08 - val_loss: 5.2304 - val_acc: 0.0000e+00\n",
      "Epoch 265/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5081 - acc: 3.6210e-08 - val_loss: 5.2209 - val_acc: 0.0000e+00\n",
      "Epoch 266/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5079 - acc: 3.8116e-08 - val_loss: 5.2245 - val_acc: 0.0000e+00\n",
      "Epoch 267/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5080 - acc: 3.6210e-08 - val_loss: 5.2305 - val_acc: 0.0000e+00\n",
      "Epoch 268/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5080 - acc: 3.6210e-08 - val_loss: 5.2240 - val_acc: 0.0000e+00\n",
      "Epoch 269/1000\n",
      " 18432/254220 [=>............................] - ETA: 2:06 - loss: 5.5864 - acc: 2.6286e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5073 - acc: 3.6210e-08 - val_loss: 5.2303 - val_acc: 0.0000e+00\n",
      "Epoch 287/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5070 - acc: 3.4305e-08 - val_loss: 5.2741 - val_acc: 0.0000e+00\n",
      "Epoch 288/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5072 - acc: 3.4305e-08 - val_loss: 5.2247 - val_acc: 0.0000e+00\n",
      "Epoch 289/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5067 - acc: 3.4305e-08 - val_loss: 5.2214 - val_acc: 0.0000e+00\n",
      "Epoch 290/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5076 - acc: 3.6210e-08 - val_loss: 5.2222 - val_acc: 0.0000e+00\n",
      "Epoch 291/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5072 - acc: 3.8116e-08 - val_loss: 5.2279 - val_acc: 0.0000e+00\n",
      "Epoch 292/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5069 - acc: 3.6210e-08 - val_loss: 5.2451 - val_acc: 0.0000e+00\n",
      "Epoch 293/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5067 - acc: 3.8116e-08 - val_loss: 5.2441 - val_acc: 0.0000e+00\n",
      "Epoch 294/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5072 - acc: 3.4305e-08 - val_loss: 5.2203 - val_acc: 0.0000e+00\n",
      "Epoch 295/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5074 - acc: 3.8116e-08 - val_loss: 5.2235 - val_acc: 0.0000e+00\n",
      "Epoch 296/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5064 - acc: 3.6210e-08 - val_loss: 5.2378 - val_acc: 0.0000e+00\n",
      "Epoch 297/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5068 - acc: 3.6210e-08 - val_loss: 5.2406 - val_acc: 0.0000e+00\n",
      "Epoch 298/1000\n",
      " 47104/254220 [====>.........................] - ETA: 1:53 - loss: 5.5271 - acc: 2.0571e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5062 - acc: 3.8116e-08 - val_loss: 5.2247 - val_acc: 0.0000e+00\n",
      "Epoch 317/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5063 - acc: 3.8116e-08 - val_loss: 5.2241 - val_acc: 0.0000e+00\n",
      "Epoch 318/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5061 - acc: 3.4305e-08 - val_loss: 5.2264 - val_acc: 0.0000e+00\n",
      "Epoch 319/1000\n",
      "254220/254220 [==============================] - 138s 545us/step - loss: 5.5059 - acc: 3.6210e-08 - val_loss: 5.2261 - val_acc: 0.0000e+00\n",
      "Epoch 320/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5059 - acc: 3.2399e-08 - val_loss: 5.2438 - val_acc: 0.0000e+00\n",
      "Epoch 321/1000\n",
      "254220/254220 [==============================] - 139s 549us/step - loss: 5.5061 - acc: 3.6210e-08 - val_loss: 5.2378 - val_acc: 0.0000e+00\n",
      "Epoch 322/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5055 - acc: 4.0022e-08 - val_loss: 5.2253 - val_acc: 0.0000e+00\n",
      "Epoch 323/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5061 - acc: 3.8116e-08 - val_loss: 5.2286 - val_acc: 0.0000e+00\n",
      "Epoch 324/1000\n",
      "254220/254220 [==============================] - 139s 549us/step - loss: 5.5059 - acc: 3.6210e-08 - val_loss: 5.2274 - val_acc: 0.0000e+00\n",
      "Epoch 325/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5065 - acc: 3.8116e-08 - val_loss: 5.2244 - val_acc: 0.0000e+00\n",
      "Epoch 326/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5061 - acc: 3.6210e-08 - val_loss: 5.2392 - val_acc: 0.0000e+00\n",
      "Epoch 327/1000\n",
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5064 - acc: 3.6210e-08 - val_loss: 5.2360 - val_acc: 0.0000e+00\n",
      "Epoch 328/1000\n",
      " 93184/254220 [=========>....................] - ETA: 1:27 - loss: 5.5199 - acc: 4.6794e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5051 - acc: 3.6210e-08 - val_loss: 5.2299 - val_acc: 0.0000e+00\n",
      "Epoch 346/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5049 - acc: 3.6210e-08 - val_loss: 5.2413 - val_acc: 0.0000e+00\n",
      "Epoch 347/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5056 - acc: 3.6210e-08 - val_loss: 5.2641 - val_acc: 0.0000e+00\n",
      "Epoch 348/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5047 - acc: 4.0022e-08 - val_loss: 5.2330 - val_acc: 0.0000e+00\n",
      "Epoch 349/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5048 - acc: 3.8116e-08 - val_loss: 5.2261 - val_acc: 0.0000e+00\n",
      "Epoch 350/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5051 - acc: 3.8116e-08 - val_loss: 5.2236 - val_acc: 0.0000e+00\n",
      "Epoch 351/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5055 - acc: 3.6210e-08 - val_loss: 5.2362 - val_acc: 0.0000e+00\n",
      "Epoch 352/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5052 - acc: 3.6210e-08 - val_loss: 5.2304 - val_acc: 0.0000e+00\n",
      "Epoch 353/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5050 - acc: 3.4305e-08 - val_loss: 5.2277 - val_acc: 0.0000e+00\n",
      "Epoch 354/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5048 - acc: 4.0022e-08 - val_loss: 5.2625 - val_acc: 0.0000e+00\n",
      "Epoch 355/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5050 - acc: 3.8116e-08 - val_loss: 5.2267 - val_acc: 0.0000e+00\n",
      "Epoch 356/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5052 - acc: 4.0022e-08 - val_loss: 5.2523 - val_acc: 0.0000e+00\n",
      "Epoch 357/1000\n",
      "199680/254220 [======================>.......] - ETA: 29s - loss: 5.5009 - acc: 3.6395e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5044 - acc: 3.6210e-08 - val_loss: 5.2272 - val_acc: 0.0000e+00\n",
      "Epoch 376/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5049 - acc: 3.6210e-08 - val_loss: 5.2257 - val_acc: 0.0000e+00\n",
      "Epoch 377/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5044 - acc: 3.8116e-08 - val_loss: 5.2291 - val_acc: 0.0000e+00\n",
      "Epoch 378/1000\n",
      "254220/254220 [==============================] - 140s 549us/step - loss: 5.5044 - acc: 4.0022e-08 - val_loss: 5.2493 - val_acc: 0.0000e+00\n",
      "Epoch 379/1000\n",
      "254220/254220 [==============================] - 139s 546us/step - loss: 5.5041 - acc: 3.6210e-08 - val_loss: 5.2284 - val_acc: 0.0000e+00\n",
      "Epoch 380/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5045 - acc: 3.8116e-08 - val_loss: 5.2349 - val_acc: 0.0000e+00\n",
      "Epoch 381/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5040 - acc: 3.6210e-08 - val_loss: 5.2266 - val_acc: 0.0000e+00\n",
      "Epoch 382/1000\n",
      "254220/254220 [==============================] - 140s 550us/step - loss: 5.5046 - acc: 3.6210e-08 - val_loss: 5.2296 - val_acc: 0.0000e+00\n",
      "Epoch 383/1000\n",
      "254220/254220 [==============================] - 139s 548us/step - loss: 5.5047 - acc: 3.8116e-08 - val_loss: 5.2393 - val_acc: 0.0000e+00\n",
      "Epoch 384/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5045 - acc: 4.0022e-08 - val_loss: 5.2331 - val_acc: 0.0000e+00\n",
      "Epoch 385/1000\n",
      "254220/254220 [==============================] - 139s 545us/step - loss: 5.5042 - acc: 3.8116e-08 - val_loss: 5.2303 - val_acc: 0.0000e+00\n",
      "Epoch 386/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5039 - acc: 3.6210e-08 - val_loss: 5.2239 - val_acc: 0.0000e+00\n",
      "Epoch 387/1000\n",
      "238592/254220 [===========================>..] - ETA: 8s - loss: 5.5036 - acc: 3.4521e-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254220/254220 [==============================] - 138s 541us/step - loss: 5.5036 - acc: 3.8116e-08 - val_loss: 5.2291 - val_acc: 0.0000e+00\n",
      "Epoch 397/1000\n",
      "254220/254220 [==============================] - 139s 547us/step - loss: 5.5047 - acc: 4.0022e-08 - val_loss: 5.2265 - val_acc: 0.0000e+00\n",
      "Epoch 398/1000\n",
      "154624/254220 [=================>............] - ETA: 54s - loss: 5.5127 - acc: 2.1934e-08"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-daca09bc8daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m               \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m              )\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/device:GPU:2'):\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    #from keras.layers import Dense, Dropout, Flatten\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose,Activation\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras import backend as K\n",
    "    import pickle as pk\n",
    "    batch_size = 1024\n",
    "    epochs = 100\n",
    "    import numpy as  np\n",
    "    import os\n",
    "    cwd = os.getcwd()\n",
    "    #x_train = os.path.join(cwd,'Noisy_TCDTIMIT/Babble/20/volunteers/01M/straightcam')\n",
    "    #y_train = os.path.join(cwd,'Clean/volunteers/01M/straightcam')\n",
    "\n",
    "    pickle_train = open(\"noiseBabble_xtrain.pickle\",\"rb\")\n",
    "    x_train = pk.load(pickle_train)\n",
    "    pickle_trainlabel = open(\"cleanBabble.pickle\",\"rb\")\n",
    "    y_train = pk.load(pickle_trainlabel)\n",
    "\n",
    "    print(\"Got the input data\")\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    x_train = x_train.reshape(x_train.shape[0], 129, 16, 1)\n",
    "    y_train = y_train.reshape(y_train.shape[0], 129, 16, 1)\n",
    "    print(np.asarray(x_train).shape)\n",
    "\n",
    "    model = Sequential()\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size=(7, 7),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros',input_shape=(129,16,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (5, 5),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3, 3),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (1, 1),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    #model.add(Conv2DTranspose(128, (3, 3), activation='relu',padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2DTranspose(128, (3, 3),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2DTranspose(64, (5, 5),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2DTranspose(1, (7, 7),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dense(128, activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.mean_squared_error,\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    #model.build()\n",
    "\n",
    "    print(model.summary())\n",
    "    model.fit(np.asarray(x_train), np.asarray(y_train),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split = 0.01\n",
    "             )\n",
    "\n",
    "    model.save_weights('weights_1.h5')\n",
    "\n",
    "    #score = model.evaluate(x_train, y_train, verbose=0)\n",
    "\n",
    "    #print('Test loss:', score[0])\n",
    "    #print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "pickle_in = open(\"noiseBabble.pickle\",\"rb\")\n",
    "x = pk.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1540728, 129, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.asarray(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "256788\n",
      "513576\n",
      "770364\n",
      "1027152\n",
      "1283940\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,1540728,256788):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256789"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1283940-1027152+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = open(\"noiseBabble_15db.pickle\",\"wb\")\n",
    "pk.dump(x[0:256788],pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = open(\"noiseBabble_5db.pickle\",\"wb\")\n",
    "pk.dump(x[256788:513576],pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = open(\"noiseBabble_10db.pickle\",\"wb\")\n",
    "pk.dump(x[513576:770364],pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = open(\"noiseBabble_-5db.pickle\",\"wb\")\n",
    "pk.dump(x[770364:1027152],pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = open(\"noiseBabble_20db.pickle\",\"wb\")\n",
    "pk.dump(x[1027152:1283940],pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = open(\"noiseBabble_0db.pickle\",\"wb\")\n",
    "pk.dump(x[1283940:1540728],pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the input data\n",
      "(1000, 129, 16, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 123, 10, 64)       3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 123, 10, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 123, 10, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 119, 6, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 119, 6, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 119, 6, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 117, 4, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 117, 4, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 117, 4, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 117, 4, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 117, 4, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 117, 4, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 119, 6, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 119, 6, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 119, 6, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 123, 10, 64)       204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 123, 10, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 123, 10, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 129, 16, 1)        3137      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 129, 16, 1)        4         \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 129, 16, 1)        0         \n",
      "=================================================================\n",
      "Total params: 1,075,717\n",
      "Trainable params: 1,073,923\n",
      "Non-trainable params: 1,794\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 990 samples, validate on 10 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_5/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_1/Adam/gradients/conv2d_5/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_1/Adam/gradients/conv2d_5/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_5/kernel/read)]]\n\t [[{{node metrics_1/acc/Mean_1/_783}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2748_metrics_1/acc/Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0dfad9ad3c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m               \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m              )\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_5/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_1/Adam/gradients/conv2d_5/convolution_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_1/Adam/gradients/conv2d_5/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_5/kernel/read)]]\n\t [[{{node metrics_1/acc/Mean_1/_783}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2748_metrics_1/acc/Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/device:GPU:2'):\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    #from keras.layers import Dense, Dropout, Flatten\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose,Activation\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras import backend as K\n",
    "    import pickle as pk\n",
    "    batch_size = 1024\n",
    "    epochs = 50\n",
    "    import numpy as  np\n",
    "    \n",
    "    pickle_train = open(\"noiseBabble_20db.pickle\",\"rb\")\n",
    "    x_train = pk.load(pickle_train)\n",
    "    pickle_trainlabel = open(\"cleanBabble.pickle\",\"rb\")\n",
    "    y_train = pk.load(pickle_trainlabel)\n",
    "\n",
    "    print(\"Got the input data\")\n",
    "    x_train = np.asarray(x_train[0:1000])\n",
    "    y_train = np.asarray(y_train[0:1000])\n",
    "    x_train = x_train.reshape(x_train.shape[0], 129, 16, 1)\n",
    "    y_train = y_train.reshape(y_train.shape[0], 129, 16, 1)\n",
    "    print(np.asarray(x_train).shape)\n",
    "\n",
    "    model = Sequential()\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size=(7, 7),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros',input_shape=(129,16,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (5, 5),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3, 3),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (1, 1),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2DTranspose(128, (3, 3),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2DTranspose(64, (5, 5),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2DTranspose(1, (7, 7),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    \n",
    "    model.compile(loss=keras.losses.mean_squared_error,\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999),\n",
    "                  metrics=['accuracy'])\n",
    "    #model.build()\n",
    "\n",
    "    print(model.summary())\n",
    "    model.fit(np.asarray(x_train), np.asarray(y_train),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              shuffle=True,\n",
    "              validation_split = 0.01\n",
    "             )\n",
    "\n",
    "    model.save_weights('weights_babble_20db.h5')\n",
    "    model.save('model_babble_20db.h5')\n",
    "    #score = model.evaluate(x_train, y_train, verbose=0)\n",
    "\n",
    "    #print('Test loss:', score[0])\n",
    "    #print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/media/hd2/chaitanya/Deep_Learning/project/TCDTIMIT/Noisy_TCDTIMIT/Babble/15/lipspeakers/Lipspkr2/straightcam')\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "sample_rate, samples = wavfile.read('si1139.wav')\n",
    "NoiseBabble= []\n",
    "Phase = []\n",
    "f, t, Zxx = signal.stft(samples, sample_rate,nperseg=256,nfft=256)\n",
    "phase = np.angle(Zxx)\n",
    "T = len(t)//16\n",
    "for i in range(0,(T)*16,16):\n",
    "  NoiseBabble.append(np.log(np.abs(Zxx[:,i:i+16])+1e-8))\n",
    "  Phase.append(phase[:,i:i+16])\n",
    "NoiseBabble = np.asarray(NoiseBabble)\n",
    "NoiseBabble = NoiseBabble.reshape(NoiseBabble.shape[0], 129, 16, 1)\n",
    "b = np.asarray(NoiseBabble)\n",
    "rectified = model.predict(b)\n",
    "rectified =rectified.reshape(rectified.shape[0], 129, 16)\n",
    "A = np.zeros((129,0))\n",
    "for i in range(T):\n",
    "  S = np.exp(rectified[i])\n",
    "  A = np.append(A,S*np.cos(Phase[i]) + 1j * S*np.sin(Phase[i]),axis=1)\n",
    "t,x = signal.istft(A,nperseg=256,nfft=256)\n",
    "x = np.asarray(x, dtype=np.int16)\n",
    "#data=np.int16(x/np.max(np.abs(x)) * 32767)\n",
    "wavfile.write('Test3.wav', 16000, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
