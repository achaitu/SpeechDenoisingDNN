{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the input data\n",
      "(100000, 129, 16)\n",
      "H (99993, 129, 128)\n",
      "H2 (99993, 129, 128, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 123, 41, 64)       3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 123, 41, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 123, 41, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 119, 13, 128)      204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 119, 13, 128)      512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 119, 13, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 117, 11, 256)      295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 117, 11, 256)      1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 117, 11, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 117, 4, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 117, 4, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 117, 4, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 119, 6, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 119, 6, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 119, 6, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 123, 10, 64)       204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 123, 10, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 123, 10, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 129, 16, 1)        3137      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 129, 16, 1)        4         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 129, 16, 1)        0         \n",
      "=================================================================\n",
      "Total params: 1,075,717\n",
      "Trainable params: 1,073,923\n",
      "Non-trainable params: 1,794\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 98993 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "84224/98993 [========================>.....] - ETA: 12:07 - loss: 6.3419 - acc: 1.7257e-08"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-398a9fd232da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m               \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m              )\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#with tf.device('/device:GPU:2'):\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose,Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "pickle_train = open(\"noiseBabble_20db.pickle\",\"rb\")\n",
    "x_train = pk.load(pickle_train)\n",
    "pickle_trainlabel = open(\"cleanBabble.pickle\",\"rb\")\n",
    "y_train = pk.load(pickle_trainlabel)\n",
    "\n",
    "print(\"Got the input data\")\n",
    "x_train = np.asarray(x_train[0:100000])\n",
    "y_train = np.asarray(y_train[0:100000])\n",
    "print(np.asarray(x_train).shape)\n",
    "#     print(x_train[0:10].shape)\n",
    "X_train = []\n",
    "#X_train = np.asarray(X_train)\n",
    "for i in range(7,len(x_train)):\n",
    "    temp = x_train[i-7]\n",
    "    for j in range(1,8):\n",
    "        temp = np.concatenate((temp,x_train[(i-7)+j]),axis=1)\n",
    "    X_train.append(temp)\n",
    "    #X_train.append(x_train[i-8:i,:].reshape(129,128))\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "print(\"H\",np.asarray(X_train).shape)\n",
    "X_train = X_train.reshape(X_train.shape[0], 129, 128, 1)\n",
    "print(\"H2\",np.asarray(X_train).shape)\n",
    "y_train = y_train.reshape(y_train.shape[0], 129, 16, 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(7, 7),strides=(1, 3),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros',input_shape=(129,128,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (5, 5),strides=(1, 3),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256, (3, 3),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(256, (1, 1),strides=(1, 3),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "#model.add(Conv2DTranspose(128, (3, 3), activation='relu',padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2DTranspose(128, (3, 3),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2DTranspose(64, (5, 5),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2DTranspose(1, (7, 7),padding='valid',use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=['accuracy'])\n",
    "#model.build()\n",
    "\n",
    "print(model.summary())\n",
    "with tf.device('/device:GPU:2') as open:\n",
    "    model.fit(np.asarray(X_train), np.asarray(y_train[7:]),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split = 0.01\n",
    "             )\n",
    "\n",
    "model.save_weights('weights_8frames_5.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#model.save('model_8frames_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('model_8frames_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "82944\n",
      "(82944,)\n",
      "(82944,)\n",
      "Z (129, 649)\n",
      "T 40\n",
      "N (40, 129, 16)\n",
      "S (40, 129, 16)\n",
      "tempshape (129, 32)\n",
      "tempshape (129, 48)\n",
      "tempshape (129, 64)\n",
      "tempshape (129, 80)\n",
      "tempshape (129, 96)\n",
      "tempshape (129, 112)\n",
      "tempshape (129, 128)\n",
      "tempshape (129, 32)\n",
      "tempshape (129, 48)\n",
      "tempshape (129, 64)\n",
      "tempshape (129, 80)\n",
      "tempshape (129, 96)\n",
      "tempshape (129, 112)\n",
      "tempshape (129, 128)\n",
      "tempshape (129, 32)\n",
      "tempshape (129, 48)\n",
      "tempshape (129, 64)\n",
      "tempshape (129, 80)\n",
      "tempshape (129, 96)\n",
      "tempshape (129, 112)\n",
      "tempshape (129, 128)\n",
      "tempshape (129, 32)\n",
      "tempshape (129, 48)\n",
      "tempshape (129, 64)\n",
      "tempshape (129, 80)\n",
      "tempshape (129, 96)\n",
      "tempshape (129, 112)\n",
      "tempshape (129, 128)\n",
      "tempshape (129, 32)\n",
      "tempshape (129, 48)\n",
      "tempshape (129, 64)\n",
      "tempshape (129, 80)\n",
      "tempshape (129, 96)\n",
      "tempshape (129, 112)\n",
      "tempshape (129, 128)\n",
      "tempshape (129, 32)\n",
      "tempshape (129, 48)\n",
      "tempshape (129, 64)\n",
      "tempshape (129, 80)\n",
      "tempshape (129, 96)\n",
      "tempshape (129, 112)\n",
      "tempshape (129, 128)\n",
      "tempshape (129, 32)\n",
      "tempshape (129, 48)\n",
      "tempshape (129, 64)\n",
      "tempshape (129, 80)\n",
      "tempshape (129, 96)\n",
      "tempshape (129, 112)\n",
      "tempshape (129, 128)\n",
      "tempshape (129, 32)\n",
      "tempshape (129, 48)\n",
      "tempshape (129, 64)\n",
      "tempshape (129, 80)\n",
      "tempshape (129, 96)\n",
      "tempshape (129, 112)\n",
      "tempshape (129, 128)\n",
      "H1 (40, 129, 128)\n",
      "R (40, 129, 16)\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "sample_rate, samples = wavfile.read('/media/hd2/chaitanya/Deep_Learning/project/TCDTIMIT/Noisy_TCDTIMIT/Babble/20/volunteers/01M/straightcam/sa1.wav')\n",
    "print(sample_rate)\n",
    "print(len(samples))\n",
    "print(samples.shape)\n",
    "#samples = samples.reshape(samples.shape[0])\n",
    "#samples = samples[:,1]\n",
    "print((samples).shape)\n",
    "NoiseBabble= []\n",
    "Phase = []\n",
    "f, t, Zxx = signal.stft(samples, sample_rate,nperseg=256,nfft=256)\n",
    "print(\"Z\",Zxx.shape)\n",
    "phase = np.angle(Zxx)\n",
    "T = len(t)//16\n",
    "print(\"T\",T)\n",
    "for i in range(0,(T)*16,16):\n",
    "  NoiseBabble.append(np.log(np.abs(Zxx[:,i:i+16])+1e-8))\n",
    "  Phase.append(phase[:,i:i+16])\n",
    "print(\"N\",np.asarray(NoiseBabble).shape)\n",
    "# if len(t) % 16 !=0:\n",
    "#   NoiseBabble.append(np.log(np.abs(Zxx[:,len(t)-16:len(t)])+1e-8))\n",
    "#   Phase.append(phase[:,len(t)-16:len(t)])\n",
    "#   T = T+1\n",
    "X_NoiseBabble = []\n",
    "NoiseBabble = np.asarray(NoiseBabble)\n",
    "print(\"S\",NoiseBabble.shape)\n",
    "for i in range(0,8):\n",
    "    temp = NoiseBabble[0]\n",
    "    for j in range(1,8):\n",
    "        temp = np.concatenate((temp,NoiseBabble[j]),axis=1)\n",
    "        print(\"tempshape\",temp.shape)\n",
    "    X_NoiseBabble.append(temp)\n",
    "    #X_NoiseBabble = np.append(X_NoiseBabble,NoiseBabble[0:8],axis=1)\n",
    "for i in range(8,len(NoiseBabble)):\n",
    "    temp = NoiseBabble[i]\n",
    "    for j in range(1,8):\n",
    "        temp = np.concatenate((temp,NoiseBabble[(i-8)+j]),axis=1)\n",
    "    X_NoiseBabble.append(temp)\n",
    "    #X_NoiseBabble.append(NoiseBabble[i-8:i,:,:].reshape(129,128))\n",
    "print(\"H1\",np.asarray(X_NoiseBabble).shape)\n",
    "b = np.asarray(X_NoiseBabble)\n",
    "X_NoiseBabble = b.reshape(b.shape[0], 129, 128, 1)\n",
    "\n",
    "rectified = new_model.predict(X_NoiseBabble)\n",
    "rectified =rectified.reshape(rectified.shape[0], 129, 16)\n",
    "print(\"R\",rectified.shape)\n",
    "A = np.zeros((129,0))\n",
    "for i in range(T):\n",
    "  S = np.exp(rectified[i])\n",
    "  A = np.append(A,S*np.cos(Phase[i]) + 1j * S*np.sin(Phase[i]),axis=1)\n",
    "t,x = signal.istft(A,nperseg=256,nfft=256)\n",
    "x = np.asarray(x, dtype=np.int16)\n",
    "#print(len(x))\n",
    "#data=np.int16(x/np.max(np.abs(x)) * 32767)\n",
    "wavfile.write('/media/hd2/chaitanya/Deep_Learning/project/TCDTIMIT/code/babble/testing/frames_2.wav', 16000, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "129*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
